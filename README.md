# AI_LLM_DEMO 🤖

这里汇集了公众号及日常技术分享中涉及的各种 AI & LLM 相关的实战 Demo 代码。

旨在通过最精简的代码，展示大模型开发中的核心概念与工程实践，方便大家“拿来即用”或进行二次开发。

## 📂 目录 (Table of Contents)

### 1. [LLM Prompt Caching (提示词缓存)](./LLM_PROMPT_CACHE_DEMO)

> 对应文章：《DeepSeek 提示词缓存实战：如何通过 Context Caching 节省 90% 成本？》

- **核心演示**: 对比开启缓存前后的 Token 消耗与响应速度。
- **技术要点**:
  - DeepSeek API 的 `prompt_cache_hit_tokens` 实测
  - 静态/动态前缀对缓存命中的影响
  - 成本计算 (1.0元 vs 0.1元)
- **快速开始**: [点击此处查看详细文档](./LLM_PROMPT_CACHE_DEMO/README.md)

---

## 🚀 如何使用

每个子文件夹都是一个独立的项目，包含各自的 `requirements.txt` 和说明文档。请进入子目录查看具体的运行指南。

## 🤝 关注作者

更多硬核技术文章与 AI 实战思路，欢迎关注公众号：**[您的公众号名称]** (或者留空)
